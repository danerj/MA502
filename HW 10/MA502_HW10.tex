\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{indentfirst}

\title{MA 502 Homework 10}
\author{Dane Johnson}

\begin{document}
\maketitle

\section{}


\begin{align}e^{tA} & = e^{tA - \lambda t I + \lambda t I} \\ &= e^{\lambda t I}e^{(A-\lambda I)t}\\
&= e^{\lambda t I}\left(I + (A-\lambda I)t + (A-\lambda I)^2 \frac{t^2}{2} + (A-\lambda I)^3\frac{t^3}{3!} + ... \right)\\
&= e^{\lambda t I }\left(I + (A-\lambda I)t + (A-\lambda I)^2 \frac{t^2}{2} + ... (A-\lambda I)^{n-1} \frac{t^{n-1}}{(n-1)!}\right)\\ &= e^{\lambda t }\left(I + (A-\lambda I)t + (A-\lambda I)^2 \frac{t^2}{2} + ... (A-\lambda I)^{n-1} \frac{t^{n-1}}{(n-1)!}\right)
\end{align}

Note that to get from the infinite sum in line (3) to the finite sum in line (4) we used the fact that for the characteristic polynomial of $A$, $p_A(x) = (x-\lambda)^n$, we have $p_A(A) = (A-\lambda I)^n = 0$ by Cayley-Hamilton. Therefore $(A-\lambda I)^m = 0$ for any $m \geq n$. Also to arrive at line (5) from line (4) we used

$$e^{\lambda t I} = I + \lambda t I +  \frac{\lambda^2 t^2}{2} I^2 + .... = I(1 + \lambda t + \frac{\lambda^2 t^2}{2} + ....) = I(e^{\lambda t})$$ We can then distribute the identity into the sum on line (5) without any change. 



\section{}

Consider the matrix

$$A = \begin{pmatrix}
1 & 3 \\ 3 & 1
\end{pmatrix} \,.$$

$\bullet$ First we fine an orthogonal matrix $O$ such that $O^TAO$ is diagonal. We will find eigenvalues and corresponding eigenvectors that are orthonormal.\\

$$ 0 = p_A(\lambda) = (1-\lambda)^2 - 9 \implies \lambda = -2,4\,.$$

Putting $\lambda = -2$ into $A-\lambda I = 0$ gives

$$A-\lambda I = \begin{pmatrix}
3& 3 \\ 3 & 3
\end{pmatrix}\hat{v} = 0.$$

Then any multiple of $\begin{pmatrix}
1 \\ -1
\end{pmatrix}$ is an eigenvector. We take the unit vector $v_1 = \frac{1}{\sqrt{2}} \begin{pmatrix}
1 \\ -1
\end{pmatrix}$. Similarly, for $\lambda = 4$, we have

$$A-\lambda I = \begin{pmatrix}
-3& 3 \\ 3 & -3
\end{pmatrix}\hat{v} = 0,$$

which means that any multiple of $\begin{pmatrix}
1 \\ -1
\end{pmatrix}$ is an eigenvector. We take the unit vector $v_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
1 \\ 1
\end{pmatrix}$. Note that $v_1$ and $v_2$ are already orthogonal vectors. Set

$$O = \begin{pmatrix}
v_2 & v_1
\end{pmatrix}= \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{pmatrix} = O^T.$$

Then $O^TO = OO^T = I$, which means that $O$ is an orthogonal matrix (and also that $O^T$ is orthogonal). But we require that $O^TAO$ is diagonal. So we multiply to get

$$O^TAO = \begin{pmatrix}
4 & 0 \\ 0 & -2
\end{pmatrix}
$$

In fact the eigenvalues of $A$ lie on the diagonal of $O^TAO$. 

$\bullet$ To compute $e^A$, rewrite $A$ as $A = ODO^T$. Then,

$$e^A = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \frac{A^4}{4!} + ....$$
$$ = I + ODO^T + \frac{(ODO^T)^2}{2!} + \frac{(ODO^T)^3}{3!} + \frac{(ODO^T)^4}{4!} + ....$$
$$ = I + ODO^T + \frac{OD^2O^T}{2!} + \frac{OD^3O^T}{3!} + \frac{OD^4O^T}{4!} + ...$$

\section{}

Define the inner product

$$\langle f, g \rangle = \int_{-1}^1 f(t)g(t)(1-t^2) \, dt$$

on the vector space of polynomials with real coefficients. We find an orthonormal basis $\{\phi_0, \phi_1, \phi_2\}$ for the subspace generated by $\{\frac{\sqrt{3}}{2}, \frac{\sqrt{15}}{2}t, t^2\}$. \\

Take $\psi_0(t) =  \frac{\sqrt{3}}{2}$. To normalize $\psi_0$, we set $$\phi_0(t) = \frac{\psi_0}{(\langle \psi_0, \psi_0\rangle)^{1/2}} = \frac{\frac{\sqrt{3}}{2}}{\sqrt{\int_{-1}^1 (\frac{\sqrt{3}}{2})^2(1-t^2)dt}}= \frac{\frac{\sqrt{3}}{2}}{1} = \frac{\sqrt{3}}{2}.$$

Next set $$\psi_1(t) = \frac{\sqrt{15}}{2}t - \langle \frac{\sqrt{15}}{2}t, \phi_0(t) \rangle \phi_0 = \frac{\sqrt{15}}{2}t - \left(\int_{-1}^1 \frac{\sqrt{15}}{2}t\frac{\sqrt{3}}{2}(1-t^2) \, dt\right)\frac{\sqrt{3}}{2} $$ $$= \frac{\sqrt{15}}{2}t - 0\frac{\sqrt{3}}{2} = \frac{\sqrt{15}}{2}t.$$ 

Then to find a normalized vector take

$$\phi_1(t) = \frac{\psi_1}{(\langle \psi_1, \psi_1\rangle)^{1/2}} = \frac{\frac{\sqrt{15}}{2}t}{\left(\int_{-1}^1 (\frac{\sqrt{15}}{2}t)^2(1-t^2) dt\right)^{1/2}} = \frac{\frac{\sqrt{15}}{2}t}{1}  = \frac{\sqrt{15}}{2}t \,.$$

Next set\begin{align*}
\psi_2(t) &= t^2 - \langle t^2, \phi_0 \rangle \phi_0 - \langle t^2, \phi_1 \rangle \phi_1\\
&= t^2 - \left(\int_{-1}^1 t^2\frac{\sqrt{3}}{2}(1-t^2) \, dt\right)\frac{\sqrt{3}}{2} - \left(\int_{-1}^1 t^2(\frac{\sqrt{15}}{2}t)(1-t^2) \, dt\right) \frac{\sqrt{15}}{2}t\\
&= t^2 - \frac{2}{5\sqrt{3}}\frac{\sqrt{3}}{2} - 0 \frac{\sqrt{15}}{2}t\\
&= t^2 -\frac{1}{5}\,.
\end{align*}

Finally, we set $$\phi_2(t) = \frac{\psi_2}{(\langle \psi_2, \psi_1\rangle)^{1/2}} = \frac{t^2 - \frac{1}{5}}{\left(\int_{-1}^1 (t^2-\frac{1}{5})^2(1-t^2) \, dt \right)^{1/2}} = \frac{t^2 - \frac{1}{5}}{\frac{4\sqrt{42}}{105}} = \frac{105t^2 - 21}{4\sqrt{42}}\,.$$

\section{}

For an $n\times n$ matrix $A$ define $\langle x, y \rangle = \sum_{i,j = 1}^n a_{ij}x_iy_j$. To determine under what conditions $\langle x, y\rangle$ defines an inner product on $\mathbb{R}^3$, we will need $n = 3$ for the definition to make sense. First note that if we write this sum in terms of matrix algebra we have

$$ \langle x, y \rangle \equiv x^TAy = \begin{pmatrix} x_1& x_2& x_3\end{pmatrix}\begin{pmatrix}
a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}\begin{pmatrix}
y_1 \\ y_2 \\ y_3 \end{pmatrix}$$

where we use the symbol '$\equiv$' only to regard the fact that the matrix product above results in a $1\times 1$ real matrix while $\sum_{i,j = 1}^n a_{ij}x_iy_j$ is a real number. Since the set of $1\times 1$ real matrices is isomorphic to the set of real numbers we can readily pass back and forth between the matrix algebra representation of $\langle x, y \rangle$ and $\sum_{i,j = 1}^n a_{ij}x_iy_j$. We consider the definition of an inner product on $\mathbb{R}^3$ in order to find information about what properties $A$ must have.\\

Let $x,y,z \in \mathbb{R}^3$ and $c \in \mathbb{R}$. \\

1)\\

To have inner product it must be that $\langle x+y, z \rangle> = \langle x,z \rangle + \langle y, z \rangle$. However, we see that

$$\langle x+y, z \rangle = (x+y)^TAz = (x^T + y^T)Az = x^TAz+y^TAz = \langle x,z \rangle + \langle y, z \rangle$$

for any $A \in \mathbb{R}^{3\times 3}$, so we gain no information.\\

2)\\

Another requirement for an inner product is that $\langle cx, y \rangle = c\langle x , y \rangle$. However, we see that

$$\langle cx, y \rangle = (cx)^TAy = cx^TAy = c\langle x, y \rangle$$

for any $A$ and again gain no helpful information.\\

3) To have an inner product we require $\langle x, y \rangle = \langle y, x \rangle$. If $A$ is symmetric, then $A = A^T$ and remembering that since $x^TAy$ is a $1 \times 1$ matrix, it is also symmetric we have 

$$\langle x, y \rangle =  x^TAy = (x^TAy)^T = (x^TA^Ty)^T = y^TAx = \langle y, x \rangle \,.$$

Also, if it is the case that $\langle x, y = \sum_{i,j = 1}^n a_{ij}x_iy_j = \sum_{i,j = 1}^n a_{ij}y_ix_j = \langle y, x \rangle$ then this means the sum is the same even if $i$ and $j$ are reversed. That is, $a_{ij} = a_{ji}$ so that $A$ must be symmetric. Therefore, it is a necessary and sufficient condition that $A$ is symmetric.\\

4) We require that $\langle x, x \rangle \geq 0$ for any $x$ and $\langle x, x \rangle = 0$ if and only if $x = 0$. Here this meanst that $x^TAx \geq 0$ for any $x \in \mathbb{R}^3$ and $x^TAx = 0$ if and only if $x = 0$. But this is the definition of positive definiteness of $A$ (since we have already established that $A$ must be symmetric it is acceptable to discuss positive definiteness). Since definition are bidirectional, the final requirement on $\langle \cdot, \cdot \rangle$ is satisfied if and only if $A$ is a positive definite matrix. \\

{\bf The conclusion is that $\langle x, y \rangle = \sum_{i,j = 1}^n a_{ij}x_iy_j$ defines an inner product on $\mathbb{R}^3$ if and only if $A$ is a symmetric matrix that is also positive definite.}

\section{}

The system

$$Ax = \begin{pmatrix} 2&0\\-1&1\\0&2 \end{pmatrix}b = \begin{pmatrix}
1\\0\\-1 \end{pmatrix}$$
has no solution. To see this, we use Gaussian elimination on an augmented matrix:

$$\begin{bmatrix}
2 & 0 & 1 \\ -1 & 1 & 0 \\ 0 & 2 & -1 \end{bmatrix} \sim \begin{bmatrix}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0& 0 & 1 \end{bmatrix} \,.$$

The second and third row of this augment matrix shows that if $x = (x_1, x_2)$ is a solution to $Ax = b$, then $x_2 = 0$ and also that $0x_2 = 1$. But this would imply that $0 = 1$. Therefore, no $x \in \mathbb{R}^2$ can satisfy the system. \\

We instead seek an approximate solution $A\hat{x} \approx b$ using least squares approximation.

The system $A^TA\hat{x} = A^Tb$ will have a unique solution $\hat{x}$ and $\hat{x}$ minimizes $||Ax-b||^2$. We have,

$$A^TA\hat{x} = A^Tb$$
$$\begin{pmatrix} 2&-1&0 \\ 0&1 &2 \end{pmatrix}
\begin{pmatrix} 2 & 0 \\ -1 & 1 \\ 0 & 2 \end{pmatrix}\hat{x} = \begin{pmatrix} 2&-1&0 \\ 0&1 &2 \end{pmatrix} \begin{pmatrix} 1\\0\\-1 \end{pmatrix}$$

$$\begin{pmatrix} 5&-1 \\ -1&5\end{pmatrix} \hat{x} =
\begin{pmatrix} 2 \\ -2 \end{pmatrix}$$

$$\hat{x} = \begin{pmatrix} 5&-1 \\ -1&5\end{pmatrix}^{-1} \begin{pmatrix} 2 \\ -2 \end{pmatrix} = \begin{pmatrix}
1/3 \\ -1/3 \end{pmatrix} \,.$$

Our approximation gives

$$A\hat{x} = \begin{pmatrix} 2 & 0 \\ -1 & 1 \\ 0 & 2 \end{pmatrix}
\begin{pmatrix}
1/3 \\ -1/3 \end{pmatrix} = \begin{pmatrix}
2/3 \\ -2/3 \\ -2/3
\end{pmatrix} \,. $$

\end{document}